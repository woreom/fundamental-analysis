import pandas as pd
import numpy as np
import os
from datetime import date, timedelta
import math
import glob
import xlwt
import csv

from datetime import datetime
from typing import List, Tuple, Optional, Dict

import warnings
import sys
if not sys.warnoptions:
    warnings.simplefilter("ignore")
    os.environ["PYTHONWARNINGS"] = "ignore"
    print("*** Be careful no warnings are printed ***")
    
    

def read_forexfactory_data(path:str, files_name:str) -> pd.DataFrame:
    '''
    The read_forexfactory_data function reads an Excel file containing Forex Factory data located in the given path with the given file name.
    It then extracts the date from the 'History' column and sets it as the index for the DataFrame.
    It creates a new column called 'Actual [file_name]' and sets the values from the 'Actual' column of the original DataFrame.
    It drops unnecessary columns like 'History', 'Date', 'Forecast', 'Previous', and 'Actual' columns.
    Finally, it concatenates the modified DataFrame to an empty DataFrame called 'final_df' and returns the final DataFrame.

    Parameters:
    - path (str): The directory path where the Excel file is located.
    - file_name (str): The name of the Excel file (without extension) to be read.

    Returns:
    - pd.DataFrame: A DataFrame containing the processed Forex Factory data.

    Data Processing Steps:
    - The function reads the Excel file located at 'path' with the name 'file_name+'.xlsx' into a Pandas DataFrame.
    - It extracts the 'Date' from the 'History' column and sets it as the index for the DataFrame.
    - A new column is created, named 'Actual [file_name]', which contains the values from the 'Actual' column of the original DataFrame.
    - The function then drops unnecessary columns including 'History', 'Date', 'Forecast', 'Previous', and 'Actual'.
    - The modified DataFrame is concatenated with an empty DataFrame called 'final_df'.
    - The final DataFrame containing the processed Forex Factory data is returned.

    Example Usage:
    >>> forex_data = read_forexfactory_data('/path/to/data', 'file1')
    >>> print(forex_data.head())

    Note: This function simplifies the process of reading and preprocessing Forex Factory data from Excel files for analysis or modeling.
    '''
    final_df = pd.DataFrame()
    df = pd.read_excel(path+"/"+files_name+'.xlsx')
    try:
        df['Date'] = df['History'].str.split(',').str[1]
    except: 
        df['Date'] = df['History']
    df.index = pd.to_datetime(df["Date"], format=" %Y %b %d")
    df.sort_index(axis=0, ascending=True, inplace=True)
    df["Actual"+" "+files_name] = df["Actual"]
    df = df.drop(columns=["History", "Date", "Forecast", "Previous", "Actual"])
    final_df = pd.concat([final_df, df], axis=1) 
    
    return final_df      


def convert_monthly_to_daily(final_df: pd.DataFrame) -> pd.DataFrame:
    '''
    The convert_monthly_to_daily function takes a Pandas DataFrame with monthly frequency data and returns the same data with daily frequency.
    It does so by generating a new DataFrame with daily frequency between each month and converting the monthly data to daily data.

    Parameters:
    - final_df (pd.DataFrame): A Pandas DataFrame with monthly frequency data.

    Returns:
    - pd.DataFrame: A Pandas DataFrame with daily frequency data.

    Data Processing Steps:
    - The function creates an empty dataframe called 'new_df'.
    - It then iterates through the index of the input DataFrame 'final_df', which is assumed to be a DatetimeIndex with monthly frequency.
    - For each month, the function generates daily dates between the current and next month using 'pd.date_range'.
    - It then iterates through the columns of 'final_df' and creates a new DataFrame 'daily_df' with daily frequency data for each column.
    - The daily data is generated by repeating the monthly value for each day in the month.
    - 'daily_df' is then concatenated with 'new_df' to build the daily frequency dataset.
    - The function continues this process for all months in the input DataFrame.
    - The resulting 'new_df' contains the same data as the input 'final_df' but with daily frequency.

    Example Usage:
    >>> monthly_data = pd.DataFrame({
    ...     'Date': ['2022-01-01', '2022-02-01', '2022-03-01'],
    ...     'Value': [100, 110, 120]
    ... })
    >>> monthly_data['Date'] = pd.to_datetime(monthly_data['Date'])
    >>> monthly_data.set_index('Date', inplace=True)
    >>> daily_data = convert_monthly_to_daily(monthly_data)
    >>> print(daily_data.head())

    Note: This function is useful for converting data with monthly frequency to daily frequency, which can be necessary for various time series analysis or modeling tasks.
    '''    
    new_df = pd.DataFrame()
    for i in range(len(final_df.index)-1): 
            indexes = pd.date_range(start = final_df.index[i],end = final_df.index[i+1], freq='D')
            for n in range(len(final_df.columns)):
                daily_df = pd.DataFrame({final_df.columns[n]: np.repeat(final_df[final_df.columns[n]][i], len(indexes)-1)}, index=indexes[:-1])
                new_df = pd.concat([new_df, daily_df]) 
                
    return new_df


def combine_daily_data(files:list, path:str) -> pd.DataFrame:
    '''
    The combine_daily_data function combines daily Forex Factory dataframes into one final dataframe.
    It takes in two parameters: 'files', a list of names of Excel files to read from, and 'path', the directory path where the files are located.
    The function returns a single dataframe that combines all the daily dataframes read from the specified Excel files.

    Data Processing Steps:
    - The function first creates an empty dataframe called 'combined_daily_data'.
    - It then iterates over each file name in the 'files' list and reads the Forex Factory data from the Excel file using the 'read_forexfactory_data' function.
    - The function then converts the monthly data to daily data using the 'convert_monthly_to_daily' function.
    - The resulting daily dataframe is then joined with the 'combined_daily_data' dataframe using the 'join' method.
    - Finally, the function returns the 'combined_daily_data' dataframe containing the daily Forex Factory data from all the specified Excel files.

    Parameters:
    - files (list): A list of file names (without extensions) to process.
    - path (str): The path to the directory where the Excel files are located.

    Returns:
    - pd.DataFrame: A combined dataframe containing daily Forex Factory data.

    Example Usage:
    >>> files_to_process = ['file1', 'file2']
    >>> data_path = '/path/to/excel/files'
    >>> combined_data = combine_daily_data(files_to_process, data_path)
    >>> print(combined_data.head())

    Note: This function is useful for consolidating daily Forex Factory data from multiple Excel files into a single dataframe for analysis or modeling.
    '''
    combined_daily_data = pd.DataFrame()
    for file in files:
        my_df = read_forexfactory_data(path=path, files_name=file)
        new_df = convert_monthly_to_daily(final_df=my_df)
        combined_daily_data = new_df.join(combined_daily_data)
        
    return combined_daily_data 


def read_investing_daily_data(path:str, file_name:str) -> pd.DataFrame:
    '''
    The read_investing_daily_data function reads a CSV file of investing.com daily data, cleans the data, and drops extra columns.
    It takes two parameters, 'path' and 'file_name', which specify the path and filename of the CSV file, respectively.
    The function returns a Pandas DataFrame object with the cleaned data.

    Data Cleaning Steps:
    - Reads the CSV file into a Pandas DataFrame object.
    - Attempts to convert the 'Date' column to a datetime object using different date formats.
    - Sets the 'Date' column as the index of the DataFrame.
    - Sorts the DataFrame in ascending order based on the index.
    - Creates a new column named 'file_name', which contains the 'Price' column values.
    - Drops unnecessary columns from the DataFrame, such as 'Date', 'Price', 'Open', 'High', 'Low', 'Change %', and 'Vol.'

    Parameters:
    - path (str): The path to the directory containing the CSV file.
    - file_name (str): The name of the CSV file (without the extension) to be read.

    Returns:
    - pd.DataFrame: A cleaned Pandas DataFrame containing the daily data from the CSV file.

    Example Usage:
    >>> data_path = '/path/to/data/directory'
    >>> file_to_read = 'sample_data'
    >>> daily_data = read_investing_daily_data(data_path, file_to_read)
    >>> print(daily_data.head())
               sample_data
    Date                   
    2022-01-01       100.50
    2022-01-02       101.20
    2022-01-03       102.00
    2022-01-04       104.50
    2022-01-05       105.70

    Note: In this example, the function reads daily data from a CSV file and performs data cleaning steps to obtain a Pandas DataFrame with the desired format.
    '''
    df = pd.read_csv(path+'/'+file_name+'.csv')
    #df = pd.read_excel(path+'/'+file_name+'.xlsx')
    try:
        df.index = pd.to_datetime(df["Date"], format="%b %d, %Y")
    except:
        try:
            df.index = pd.to_datetime(df["Date"], format="%m/%d/%Y") 
        except:
            df.index = pd.to_datetime(df["Date"], format="%d/%m/%Y") 
            
    df.sort_index(axis=0, ascending=True, inplace=True)
    df[file_name] = df["Price"]  
    
    try:
        df = df.drop(columns=["Date", "Price", "Open", "High", "Low", "Change %", "Vol."])
    except:
        df = df.drop(columns=["Date", "Price", "Open", "High", "Low", "Change %"]) 
        
    return df


def return_files_name(path:str):
    '''
    The return_files_name function returns the names of all files in a specified directory.

    Parameters:
    - path (str): The path to the directory for which you want to retrieve the file names.

    Returns:
    - list: A list of file names in the specified directory.

    Example Usage:
    >>> directory_path = '/path/to/directory'
    >>> files_in_directory = return_files_name(directory_path)
    >>> print(files_in_directory)
    ['file1.txt', 'file2.csv', 'file3.xlsx']

    Note: This function is useful for obtaining a list of file names in a directory, which can be used for various file-related operations.
    '''
    files = os.listdir(path)
    return files


def monthly_features(files: list, path='../../data/fund_model/monthly'):
    """
    For this function we have 2 options:
    first option: classify features into 6 classes
    second option: classify features into 2 classes
    Generate monthly features from a list of Excel files.

    This function reads Excel files containing financial data, processes them,
    and extracts specific features for analysis.

    Parameters:
        files (list): A list of file names (without extensions) to process.
        path (str, optional): The path to the directory where the Excel files are located. Default is '../../data/fund_model/monthly'.

    Returns:
        pd.DataFrame: A DataFrame containing the processed data with features.

    Features:
        - The function calculates a binary feature 'Actual_[file_name]' for each file in the 'files' list based on the relationship between 'Actual' and 'Forecast' values.
          - If 'Actual' is greater than 'Forecast', 'Actual_[file_name]' is set to 1.
          - If 'Actual' is less than 'Forecast', 'Actual_[file_name]' is set to 0.

    Example Usage:
        files_to_process = ['file1', 'file2']
        features_df = monthly_features(files_to_process)

    """
    final_df = pd.DataFrame()    
    for files_name in files:
        df = pd.read_excel(path+'/'+files_name+'.xlsx')
        df["Actual"+" "+files_name] = np.nan
        try:
            df['Date'] = df['History'].str.split(',').str[1]
        except:
            df['Date'] = df['History']
        df.index = pd.to_datetime(df["Date"], format=" %Y %b %d")
        df.sort_index(axis=0, ascending=True, inplace=True)
        
        for i in range(len(df['Actual'])):
            # if df['Actual'][i] > df['Forecast'][i] and df['Actual'][i] > df["Previous"][i]:
            #     df["Actual"+" "+files_name][i] = 1
            # elif df['Actual'][i] < df['Forecast'][i] and df['Actual'][i] < df["Previous"][i]:    
            #     df["Actual"+" "+files_name][i] = 2
            # elif df["Actual"][i] < df["Forecast"][i] and df["Actual"][i] > df["Previous"][i]:
            #     df["Actual"+" "+files_name][i] = 3
            # elif df["Actual"][i] > df["Forecast"][i] and df["Actual"][i] < df["Previous"][i]: 
            #     df["Actual"+" "+files_name][i] = 4
            # elif df["Actual"][i] == df["Forecast"][i] and df["Actual"][i] != df["Previous"][i]:   
            #     df["Actual"+" "+files_name][i] = 5
            # else:
            #     df["Actual"+" "+files_name][i] = 6
            if df['Actual'][i] > df['Forecast'][i]:
                df["Actual"+" "+files_name][i] = 1   
            elif df['Actual'][i] < df['Forecast'][i]: 
                df["Actual"+" "+files_name][i] = 0
        df = df.drop(columns=["History", "Date", "Forecast", "Previous", "Actual"])
        df = df[~df.index.duplicated(keep='first')]
        final_df = pd.concat([df, final_df], axis=1)
        
    return final_df  


def discrete_to_continuous(df: pd.DataFrame) -> pd.DataFrame:
    '''
    The discrete_to_continuous function takes a Pandas DataFrame with discrete time intervals and interpolates it to fill in the gaps,
    returning a new DataFrame with continuous time intervals. The function works by looping through each row of the input DataFrame,
    finding the time interval between it and the next row, and then creating a new DataFrame with daily time intervals for that interval.
    It then uses forward filling to interpolate the values for the new DataFrame, and finally concatenates it with a final DataFrame that combines
    all the daily time intervals. The input DataFrame must have a datetime index, and the time intervals between rows should be of fixed length.
    The output DataFrame has a datetime index with continuous time intervals, and any missing values are filled in using forward filling.

    Parameters:
    - df (pd.DataFrame): The input Pandas DataFrame with discrete time intervals.

    Returns:
    - pd.DataFrame: A new DataFrame with continuous time intervals and interpolated values.

    Example Usage:
    >>> import pandas as pd
    >>> from datetime import datetime
    >>> data = {'Value': [10, 20, 30],
    ...         'Date': [datetime(2022, 1, 1), datetime(2022, 1, 3), datetime(2022, 1, 5)]}
    >>> df = pd.DataFrame(data)
    >>> df.set_index('Date', inplace=True)
    >>> continuous_df = discrete_to_continuous(df)
    >>> print(continuous_df)
               Value
    Date             
    2022-01-01   10.0
    2022-01-02   15.0
    2022-01-03   20.0
    2022-01-04   25.0
    2022-01-05   30.0

    Note: In this example, the function interpolates values for the continuous time intervals between the discrete time intervals in the input DataFrame.
    '''
    
    final_df = pd.DataFrame()
    for k in range(len(df.index)-1):
        daily_date=[]
        start_date = df.index[k]
        end_date = df.index[k+1]
        delta = end_date - start_date
        
        for i in range(delta.days + 1):
            day = start_date + timedelta(days=i)
            daily_date.append( pd.to_datetime(day, format=" %Y %b %d"))
        new_df = pd.DataFrame({'time':pd.Series(daily_date)})    
        new_df.index = pd.to_datetime(new_df["time"], format=" %Y %b %d")
        new_df = new_df.drop(columns=['time'])
        new_df = pd.concat([new_df, df[k:k+2]], axis=1) 
        
        for n in range(len(new_df.columns)):
            is_cell_nan = pd.isnull(new_df.iloc[0, n])
            if is_cell_nan == False:
                new_df[new_df.columns[n]] = new_df[new_df.columns[n]].fillna(method="ffill") 
        
        final_df = pd.concat([new_df, final_df])
        final_df.sort_index(axis=0, ascending=True, inplace=True)
        final_df = final_df[~final_df.index.duplicated(keep='first')]    
    final_df = final_df.fillna(0)    
   
    return final_df



def combine_investing_data(files_name:list, path:str) -> pd.DataFrame:
    '''
    The combine_investing_data function takes a list of files names and a path to a directory containing investing data files as inputs,
    and returns a combined pandas DataFrame of the daily investing data. The function first creates an empty pandas DataFrame object
    called combined_daily_data. It then loops through each file name in the input files_name list and reads in the daily investing data
    using the read_investing_daily_data function. The resulting DataFrame is then joined with the combined_daily_data DataFrame using the join method.
    After processing all the files, the final combined_daily_data DataFrame is returned, which contains the combined daily investing data.

    Parameters:
    - files_name (list): A list of file names to be processed and combined.
    - path (str): The path to the directory containing the investing data files.

    Returns:
    - pd.DataFrame: A combined DataFrame containing daily investing data from all the specified files.

    Example Usage:
    >>> file_names = ['stock_data_1.csv', 'stock_data_2.csv', 'stock_data_3.csv']
    >>> data_path = '/path/to/data/directory'
    >>> combined_data = combine_investing_data(file_names, data_path)
    >>> print(combined_data.head())
    '''
    combined_daily_data = pd.DataFrame()
    for file in files_name:
        df = read_investing_daily_data(path=path, file_name=file)
        combined_daily_data = df.join(combined_daily_data)
        
    return combined_daily_data 



def convert_str_to_float(df: pd.DataFrame) -> pd.DataFrame:
    '''
    The convert_str_to_float function takes a Pandas DataFrame as input and converts the string data types in each column to float.
    The function iterates through each column in the DataFrame, and for each column, it iterates through each row.
    If a particular cell in the DataFrame is of type string, the function removes any commas in the string to convert it to a numeric float value.
    Finally, the function converts the column to float using the astype method.
    The function modifies the input DataFrame in-place, and returns the modified DataFrame as output.
    Note that the function assumes that each string value in the DataFrame can be converted to a float after removing commas.
    If a value cannot be converted to a float, the function will raise an error.

    Parameters:
    - df (pd.DataFrame): The input Pandas DataFrame containing the data.

    Returns:
    - pd.DataFrame: The modified DataFrame with string values converted to float.

    Example Usage:
    >>> import pandas as pd
    >>> data = {'Price': ['1,000.50', '2,000.75', '3,500.25', '4,200.00', '5,800.90'],
    ...         'Quantity': ['1,000', '2,500', '3,200', '4,000', '5,100']}
    >>> df = pd.DataFrame(data)
    >>> converted_df = convert_str_to_float(df)
    >>> print(converted_df)
        Price  Quantity
    0  1000.50    1000.0
    1  2000.75    2500.0
    2  3500.25    3200.0
    3  4200.00    4000.0
    4  5800.90    5100.0

    Note: In this example, the function converts the string values in the 'Price' and 'Quantity' columns to float, removing commas in the process.
    '''
    for column in df.columns:
        for k in range(len(df[column])):
            if type(df[column][k]) == str:
                # for i in range(len(df[column])):
                    # try:
                df[column][k] = df[column][k].replace(",","")
        df[column] = df[column].astype(float)      

    return df 



def return_price(df: pd.DataFrame) -> pd.DataFrame:
    '''
    The function return_price takes a pandas DataFrame df as input and returns a new pandas DataFrame with calculated return prices.
    The function calculates the return price by using the pct_change() method on each column of the input DataFrame.
    Specifically, for each column, the method computes the percentage change between the current and a prior element, which represents the return price.
    The function creates a new pandas DataFrame named new_df with the same index as the input DataFrame.
    For each column in the input DataFrame, it creates a new column in new_df with the suffix '_return_price' added to the column name.
    The new columns contain the calculated return price for each corresponding column in the input DataFrame.
    Note that the input DataFrame must not contain any missing values. If there are any missing values, the function will return a DataFrame with missing values.
    
    Parameters:
    - df (pd.DataFrame): The input Pandas DataFrame containing the data.

    Returns:
    - pd.DataFrame: A new Pandas DataFrame with calculated return prices.

    Example Usage:
    >>> import pandas as pd
    >>> data = {'Stock_A': [100, 105, 110, 115, 120],
    ...         'Stock_B': [50, 55, 60, 65, 70]}
    >>> df = pd.DataFrame(data)
    >>> return_prices_df = return_price(df)
    >>> print(return_prices_df)
       Stock_A_return_price  Stock_B_return_price
    0                   NaN                   NaN
    1              0.050000              0.100000
    2              0.047619              0.090909
    3              0.045455              0.083333
    4              0.043478              0.076923

    Note: In this example, the function calculates the return prices for 'Stock_A' and 'Stock_B' columns in the input DataFrame.
    '''
    new_df = pd.DataFrame()
    new_df.index = df.index
    for columns in df.columns:
        new_df[columns+'_return_price'] = df[columns].pct_change()    
    return new_df    


def get_redundant_pairs(df:pd.DataFrame) -> pd.DataFrame:
    '''
    Get diagonal and lower triangular pairs of correlation matrix.
    The function iterates over the columns of the DataFrame df to obtain all possible pairs of columns.
    It then creates a set of all pairs of columns that are either on the diagonal of the correlation matrix or below it.
    Since the correlation matrix is symmetric, this ensures that we obtain all pairs of redundant columns.
    Finally, the function returns the set of redundant pairs.

    Parameters:
    - df (pd.DataFrame): The input Pandas DataFrame containing the data.

    Returns:
    - pd.DataFrame: A set of pairs of column names representing redundant pairs of columns in the DataFrame.

    Example Usage:
    >>> import pandas as pd
    >>> data = {'A': [1, 2, 3, 4, 5],
    ...         'B': [0.1, 0.2, 0.3, 0.4, 0.5],
    ...         'C': [0.5, 0.4, 0.3, 0.2, 0.1]}
    >>> df = pd.DataFrame(data)
    >>> redundant_pairs = get_redundant_pairs(df)
    >>> print(redundant_pairs)
    {('A', 'A'), ('B', 'B'), ('C', 'C'), ('B', 'A'), ('C', 'A'), ('C', 'B')}

    Note: In this example, the function returns a set of redundant pairs of columns in the DataFrame, which includes the diagonal pairs and lower triangular pairs.
    '''
    pairs_to_drop = set()
    cols = df.columns
    for i in range(0, df.shape[1]):
        for j in range(0, i+1):
            pairs_to_drop.add((cols[i], cols[j]))
    return pairs_to_drop


def get_top_abs_correlations(df:pd.DataFrame, n:int) -> pd.Series:
    '''
    The get_top_abs_correlations function takes a Pandas DataFrame and an integer n as input and returns the top n absolute correlations in the DataFrame.
    The function first calculates the correlation matrix for the DataFrame using the corr() method.
    It then calculates the absolute correlation for each pair of columns using the abs() method and unstack() function.
    Next, it drops the redundant pairs of correlations using the get_redundant_pairs() function, which returns the diagonal and
    lower triangular pairs of the correlation matrix. Finally, the function sorts the remaining pairs of correlations in descending
    order and returns the top n pairs using indexing. The output is a Pandas Series object with the top n pairs of correlations and their corresponding values.

    Parameters:
    - df (pd.DataFrame): The input Pandas DataFrame containing the data.
    - n (int): The number of top absolute correlations to retrieve.

    Returns:
    - pd.Series: A Pandas Series containing the top n pairs of correlations and their corresponding values, sorted in descending order.

    Example Usage:
    >>> import pandas as pd
    >>> data = {'A': [1, 2, 3, 4, 5],
    ...         'B': [0.1, 0.2, 0.3, 0.4, 0.5],
    ...         'C': [0.5, 0.4, 0.3, 0.2, 0.1]}
    >>> df = pd.DataFrame(data)
    >>> top_correlations = get_top_abs_correlations(df, 2)
    >>> print(top_correlations)
    A  B    0.976241
      C    0.978571
    B  C    0.976241
    dtype: float64

    Note: In this example, the function returns the top 2 absolute correlations in the DataFrame.
    '''
    au_corr = df.corr().abs().unstack()
    labels_to_drop = get_redundant_pairs(df)
    au_corr = au_corr.drop(labels=labels_to_drop).sort_values(ascending=False)
    return au_corr[0:n]



def get_top_corr_with_gold(df:pd.DataFrame, target_file_name=str) -> pd.DataFrame:
    '''
    The get_top_corr_with_gold function takes a dataframe and a target_file_name as input and returns a dataframe containing the correlation
    values between the target_file_name and all other features in the dataframe, sorted in descending order.
    The function first tries to obtain the correlation between the target_file_name and other features in the input dataframe using the corr() method,
    and if it fails, it tries to obtain the correlation between the target_file_name and features labeled with the target_file_name appended with _labeled.
    It then sorts the correlation values in descending order and stores them in a new dataframe along with the name of the corresponding feature.
    The first row is dropped since it corresponds to the target_file_name's correlation with itself. The resulting dataframe is then returned.

    Parameters:
    - df (pd.DataFrame): The input Pandas DataFrame containing the data.
    - target_file_name (str): The name of the target feature for which correlations will be calculated.

    Returns:
    - pd.DataFrame: A DataFrame containing correlation values between the target feature and other features, sorted in descending order.

    Example Usage:
    >>> import pandas as pd
    >>> data = {'A': [1, 2, 3, 4, 5],
    ...         'B': [0.1, 0.2, 0.3, 0.4, 0.5],
    ...         'C': [0.5, 0.4, 0.3, 0.2, 0.1]}
    >>> df = pd.DataFrame(data)
    >>> target_file_name = 'A'
    >>> correlation_df = get_top_corr_with_gold(df, target_file_name)
    >>> print(correlation_df)
       A       feature
    1  1       feature
    2  0  feature_labeled
    3  0             C

    Note: In this example, the target_file_name is 'A', and the function returns a DataFrame containing the correlation values between 'A' and other features.
    '''
    try:
        au_corr = df.corr()[target_file_name].abs()
    except:
        au_corr = df.corr()[target_file_name+'_labeled'].abs()
        
    au_corr = au_corr.sort_values(ascending=False)
    df = pd.DataFrame(au_corr)
    df['feature'] = df.index
    df = df.reset_index(drop=True)
    df = df.iloc[1:]
    return df



def create_new_time_features(df:pd.DataFrame) -> pd.DataFrame:
    '''
    The create_new_time_features function creates new time-based features and appends them to the input dataframe.
    The function takes in a pandas dataframe df as input, and returns the modified dataframe with two new columns: "Day of week" and "Month of year".
    The "Day of week" column indicates the day of the week for each data point in the index, where Monday is 0 and Sunday is 6.
    The "Month of year" column indicates the month of the year for each data point in the index, where January is 1 and December is 12.
    The function modifies the input dataframe by adding these two new columns and returns the modified dataframe.

    Parameters:
    - df (pd.DataFrame): The input Pandas DataFrame containing the data.

    Returns:
    - pd.DataFrame: The modified DataFrame with new time-based columns.

    Example Usage:
    >>> import pandas as pd
    >>> date_rng = pd.date_range(start='2022-01-01', end='2022-01-05', freq='D')
    >>> data = {'Value': [10, 20, 30, 40, 50]}
    >>> df = pd.DataFrame(data, index=date_rng)
    >>> modified_df = create_new_time_features(df)
    >>> print(modified_df)
                 Value  Day of week  Month of year
    2022-01-01     10            5              1
    2022-01-02     20            6              1
    2022-01-03     30            0              1
    2022-01-04     40            1              1
    2022-01-05     50            2              1
    '''
    df['Day of week'] = df.index.to_series().dt.dayofweek
    # df['Day'] = df.index.day_name()
    df['Month of year'] = df.index.to_series().dt.month
    return df


def create_nonlinear_features(df:pd.DataFrame, power_upto:int) -> pd.DataFrame:
    '''
    The create_nonlinear_features function takes in a Pandas DataFrame and an integer power_upto as input and returns a new Pandas DataFrame
    that contains nonlinear features up to the specified power. The function loops through each column in the input DataFrame and creates new columns
    with nonlinear features up to the specified power. For example, if power_upto is set to 3, for each column in the input DataFrame,
    the function creates three new columns with the original column raised to the power of 1, 2, and 3.
    The new DataFrame has the same index as the input DataFrame.
    Note: This function can be used to create polynomial features for a machine learning model.

    Parameters:
    - df (pd.DataFrame): The input Pandas DataFrame containing the data.
    - power_upto (int): The maximum power limit for the transformation. The function will create columns for powers from 1 to power_upto.

    Returns:
    - pd.DataFrame: A new DataFrame with nonlinear features up to the specified power.

    Example Usage:
    >>> import pandas as pd
    >>> data = {'A': [1, 2, 3, 4, 5],
    ...         'B': [0.1, 0.2, 0.3, 0.4, 0.5]}
    >>> df = pd.DataFrame(data)
    >>> nonlinear_features_df = create_nonlinear_features(df, 3)
    >>> print(nonlinear_features_df)
       A power1  A power2  A power3   B power1   B power2   B power3
    0         1         1         1        0.1       0.01      0.001
    1         2         4         8        0.2       0.04      0.008
    2         3         9        27        0.3       0.09      0.027
    3         4        16        64        0.4       0.16      0.064
    4         5        25       125        0.5       0.25      0.125
    '''
    new_df = pd.DataFrame()
    new_df.index = df.index
    for column in df.columns:
        for i in range(1, power_upto+1):
            new_df[column+' power'+str(i)] = df[column].pow(i)
    return new_df  


def create_nonlinear_features_with_power_Q(df:pd.DataFrame, power_upto:int) -> pd.DataFrame:
    '''
    This function takes a Pandas DataFrame df and an integer power_upto as input parameters.
    The function creates new columns in the input DataFrame by applying power transformations to each existing column up to the given power limit.
    The function iterates through each column in the DataFrame and creates new columns by raising the column values to the power of the inverse of
    the integer i for i in the range of 0 to power_upto. If i is 0, then the power becomes infinity.
    The function returns the updated DataFrame.

    Parameters:
    - df (pd.DataFrame): The input Pandas DataFrame containing the data.
    - power_upto (int): The maximum power limit for the transformation. The function will create columns for powers from 0 to power_upto.

    Returns:
    - pd.DataFrame: A new DataFrame with power-transformed versions of the original columns.

    Example Usage:
    >>> import pandas as pd
    >>> data = {'A': [1, 2, 3, 4, 5],
    ...         'B': [0.1, 0.2, 0.3, 0.4, 0.5]}
    >>> df = pd.DataFrame(data)
    >>> transformed_df = create_nonlinear_features_with_power_Q(df, 3)
    >>> print(transformed_df)
       A  B  A powerinf  B powerinf  A power1.0  B power1.0  A power0.5  B power0.5  A power0.3333333333333333  B power0.3333333333333333
    0  1  0.1        1.0        0.1         1.0         0.1    1.000000    0.316228                  1.0                  0.464158
    1  2  0.2        2.0        0.2         2.0         0.2    1.414214    0.447214                  1.0                  0.584804
    2  3  0.3        3.0        0.3         3.0         0.3    1.732051    0.547723                  1.0                  0.671582
    3  4  0.4        4.0        0.4         4.0         0.4    2.000000    0.632456                  1.0                  0.741620
    4  5  0.5        5.0        0.5         5.0         0.5    2.236068    0.707107                  1.0                  0.800000
    '''
    for column in df.columns:
        for i in range(0, power_upto+1):
            df[column+' power'+str(1/i)] = df[column].pow(1/i)
    return df 

def exp_function(df:pd.DataFrame) -> pd.DataFrame:

    '''
    This function takes a Pandas DataFrame df as an input parameter.
    The function creates new columns in the input DataFrame by applying the exponential function to each existing column.
    The function iterates through each column in the DataFrame and creates new columns by taking the exponential function of the column values.
    The function returns the updated DataFrame.

    Parameters:
    - df (pd.DataFrame): The input Pandas DataFrame containing the data.

    Returns:
    - pd.DataFrame: A new DataFrame with exponential versions of the original columns.

    Example Usage:
    >>> import pandas as pd
    >>> import numpy as np
    >>> data = {'A': [1, 2, 3, 4, 5],
    ...         'B': [0.1, 0.2, 0.3, 0.4, 0.5]}
    >>> df = pd.DataFrame(data)
    >>> exponential_df = exp_function(df)
    >>> print(exponential_df)
           A exp     B exp
    0    2.718282  1.105171
    1    7.389056  1.221403
    2   20.085537  1.349859
    3   54.598150  1.491825
    4  148.413159  1.648721
    '''
    new_df = pd.DataFrame()
    new_df.index = df.index
    for column in df.columns:
        new_df[column+' exp'] = np.exp(df[column])
    return new_df    


def labeling_target(path:str, files_name:list) -> pd.DataFrame:
    '''
    This function takes a file path and a list of file names as input parameters.
    The function reads each file from the path and performs the following operations on each file:
    1) Replaces the percentage sign (%) in the 'Change %' column with an empty string and converts the values to float data type.
    2) Labels each row in the 'Change %' column as 1 if the value is greater than or equal to 0, else labels it as 0.
    3) Converts the 'Date' column to a datetime data type using one of the specified date formats and sets it as the index of the DataFrame.
    4) Sorts the DataFrame in ascending order based on the index.
    5) Adds a new column with a name of the file name appended with '_labeled' to the DataFrame, which contains the labeled 'Change %' values.
    6) Drops unnecessary columns from the DataFrame.
    The function returns the final updated DataFrame.

    Parameters:
    - path (str): The path where the CSV files are located.
    - files_name (list): A list of file names (without the '.csv' extension) to process.

    Returns:
    - pd.DataFrame: The final DataFrame after processing all the files.

    Example Usage:
    >>> path = 'data_folder'
    >>> files_name = ['file1', 'file2', 'file3']
    >>> final_df = labeling_target(path, files_name)
    >>> print(final_df.head())
                       file1_labeled  file2_labeled  file3_labeled
    Date                                                           
    2022-01-01 00:00:00             1             1             0
    2022-01-02 00:00:00             0             1             1
    2022-01-03 00:00:00             1             0             0
    2022-01-04 00:00:00             0             0             1
    2022-01-05 00:00:00             1             1             1
    '''
    for file_name in files_name:
        df = pd.read_csv(path+'/'+file_name+'.csv')
    
        for i in range(len(df['Change %'])):
            df['Change %'][i] = float(df['Change %'][i].replace('%', ''))
            if df['Change %'][i] >= 0:
                df['Change %'][i] = 1
            else:
                df['Change %'][i] = 0


        try:
            df.index = pd.to_datetime(df["Date"], format="%b %d, %Y")
        except:
            try:
                df.index = pd.to_datetime(df["Date"], format="%m/%d/%Y") 
            except:
                df.index = pd.to_datetime(df["Date"], format="%d/%m/%Y") 

        df.sort_index(axis=0, ascending=True, inplace=True)
        df[file_name+'_labeled'] = df["Change %"]  

        try:
            df = df.drop(columns=["Date", "Price", "Open", "High", "Low", "Change %", "Vol."])
        except:
            df = df.drop(columns=["Date", "Price", "Open", "High", "Low", "Change %"]) 
    
    return df       


def lag_counter(df:pd.DataFrame, number:int) -> pd.DataFrame:
    '''
    This function takes in a pandas DataFrame 'df' and an integer 'number'.
    It returns a new DataFrame with columns representing lagged versions of the original columns, up to the number specified.
    
    Parameters:
    - df (pd.DataFrame): The input pandas DataFrame containing the data.
    - number (int): The number of lagged versions to create for each column.

    Returns:
    - pd.DataFrame: A new DataFrame with lagged columns.

    Example Usage:
    >>> import pandas as pd
    >>> data = {'A': [1, 2, 3, 4, 5],
    ...         'B': [10, 20, 30, 40, 50]}
    >>> df = pd.DataFrame(data)
    >>> lagged_df = lag_counter(df, 2)
    >>> print(lagged_df)
       Alag1  Alag2  Blag1  Blag2
    0    2.0    3.0   20.0   30.0
    1    3.0    4.0   30.0   40.0
    2    4.0    5.0   40.0   50.0
    3    NaN    NaN    NaN    NaN
    4    NaN    NaN    NaN    NaN
    '''    
    new_df = pd.DataFrame()
    for column in df.columns:
        df.rename(columns = {column:'test'}, inplace = True)
        for i in range(1, number+1):
            new_df[column+'lag'+str(i)] = df.test.shift(-i)
            # print(df.columns)
        df.rename(columns = {'test':column}, inplace = True)
    return new_df     


def create_folder(path:str, folder_name:str):
    """
    Create a new directory (folder) at the specified path.

    This function creates a new directory with the given name at the specified path. If the directory already exists,
    it prints a message indicating that the directory creation failed.

    Parameters:
        path (str): The path where the new directory should be created.
        folder_name (str): The name of the new directory to be created.

    Returns:
        None

    Example Usage:
        # Create a new folder named 'results' in the current working directory
        create_folder(os.getcwd(), 'results')
    """
    folder_path = path +'/' +folder_name
    try:
        os.makedirs(folder_path)
    except OSError:
        print ("Creation of the directory %s failed" % folder_path)
    else:
        print ("Successfully created the directory %s" % folder_path)


def count_depression_value(df:pd.DataFrame, name:str):
    '''
    Calculate the total depression value for each month in the provided DataFrame.
    In the code above, we first create an empty DataFrame monthly_totals with
    a monthly frequency datetime index that spans the entire range of the original DataFrame.

    We then loop through each month in the monthly_totals index and use a boolean mask
    to select the rows in the original DataFrame with the same year and month as
    the current loop iteration. We sum the values for each column in these selected
    rows using the sum method, and sum the resulting values across all columns
    using the sum method again. We then add the resulting total value as a new row to
    the monthly_totals DataFrame, with the index set to the current month.

    At the end of the loop, the monthly_totals DataFrame will have one column named 'total' containing
    the sum of all column values for each month in the original DataFrame, and a datetime index with one 
    row for each month. If any months have no data (i.e. missing values), we drop these rows from the 
    DataFrame using the dropna method with inplace=True.
    
    Returns:
        pd.DataFrame: A DataFrame with a datetime index containing the total depression values for each month.

    Example Usage:
        # Load a DataFrame containing monthly depression data
        depression_data = pd.read_csv('depression_data.csv')
        
        # Calculate total depression values with a custom name
        total_depression = count_depression_value(depression_data, 'CPI')   
    
    '''
    monthly_totals = pd.DataFrame(index=pd.date_range(start=df.index[0], end=df.index[-1], freq='M'))
    # Loop through each month and sum values for each column
    for month in monthly_totals.index:
        mask = (df.index.year == month.year) & (df.index.month == month.month)
        monthly_totals.loc[month, 'total_depression_'+name] = df.loc[mask, :].sum().sum()
    # Drop any rows with missing values (i.e. months with no data)
    monthly_totals.dropna(inplace=True)    
    return monthly_totals


def count_inflation_value(df:pd.DataFrame, name:str):
    '''
    Calculate the total inflation value for each month in the provided DataFrame.
    In the code above, we first create an empty DataFrame monthly_totals with
    a monthly frequency datetime index that spans the entire range of the original DataFrame.

    We then loop through each month in the monthly_totals index and use a boolean mask
    to select the rows in the original DataFrame with the same year and month as
    the current loop iteration. We sum the values for each column in these selected
    rows using the sum method, and sum the resulting values across all columns
    using the sum method again. We then add the resulting total value as a new row to
    the monthly_totals DataFrame, with the index set to the current month.

    At the end of the loop, the monthly_totals DataFrame will have one column named 'total' containing
    the sum of all column values for each month in the original DataFrame, and a datetime index with one 
    row for each month. If any months have no data (i.e. missing values), we drop these rows from the 
    DataFrame using the dropna method with inplace=True.
    

    This function calculates the total inflation value for each month in the provided DataFrame and returns a new DataFrame
    with a datetime index containing the monthly totals. It sums all column values for each month, creating a 'total_inflation_[name]'
    column in the result.

    Parameters:
        df (pd.DataFrame): The input DataFrame containing inflation data with a datetime index.
        name (str): A name or label to be appended to the 'total_inflation_' column in the result.

    Returns:
        pd.DataFrame: A DataFrame with a datetime index containing the total inflation values for each month.

    Example Usage:
        # Load a DataFrame containing monthly inflation data
        inflation_data = pd.read_csv('inflation_data.csv')
        
        # Calculate total inflation values with a custom name
        total_inflation = count_inflation_value(inflation_data, 'CPI')    
    '''    
    monthly_totals = pd.DataFrame(index=pd.date_range(start=df.index[0], end=df.index[-1], freq='M'))
    # Loop through each month and sum values for each column
    for month in monthly_totals.index:
        mask = (df.index.year == month.year) & (df.index.month == month.month)
        monthly_totals.loc[month, 'total_inflation_'+name] = df.loc[mask, :].sum().sum()
    # Drop any rows with missing values (i.e. months with no data)
    monthly_totals.dropna(inplace=True)    
    return monthly_totals


def count_monthly_price_change(df):
    """
    Count monthly price changes in a given DataFrame.

    This function takes a DataFrame with time-series data, typically representing daily prices, and calculates
    the monthly price changes as binary values (1 for a positive change and 0 for a non-positive/negative change).

    Parameters:
        df (pd.DataFrame): The input DataFrame with a datetime index and at least one numerical column
                           representing prices.

    Returns:
        pd.DataFrame: A new DataFrame with monthly price change information.

    Features:
        - The function creates a new DataFrame, 'monthly_df,' with monthly frequency and the first day of each month as the index.
        - It calculates the monthly price differences using the 'resample' method.
        - It converts the price differences to binary values, where 1 represents a positive price change, and 0 represents a non-positive/negative change.
        - The first row, which has no previous month to compare to, is removed from the result.

    Example Usage:
        # Create a DataFrame with daily price data
        daily_prices = pd.DataFrame({'Price': [100, 105, 102, 110, 108]},
                                    index=pd.date_range(start='2023-01-01', periods=5, freq='D'))

        # Count monthly price changes
        monthly_changes = count_monthly_price_change(daily_prices)
        print(monthly_changes)

    """

    # Create a new DataFrame with monthly frequency and the first day of each month as the index
    monthly_df = pd.DataFrame(index=pd.date_range(start=df.index[0], end=df.index[-1], freq='M') + pd.offsets.MonthEnd(0))
    # Calculate the monthly price differences
    monthly_df['price_diff'] = df[df.columns[0]].resample('M').last().diff(periods=1)
    # Convert the price differences to 1 (positive change) or 0 (negative change)
    monthly_df['price_change'] = (monthly_df['price_diff'] > 0).astype(int)
    # Remove the first row, which has no previous month to compare to
    monthly_df = monthly_df.iloc[1:]
    return monthly_df


def compare_depression_with_price_change(df_depression_value, df_price_change, path, file_name):
    """
    Compare depression values with monthly price changes and generate a summary CSV file.

    This function takes two DataFrames, one containing depression values and another containing monthly price changes.
    It compares the two datasets and generates a summary DataFrame, along with saving it as a CSV file.

    Parameters:
        df_depression_value (pd.DataFrame): A DataFrame containing depression values with a 'total' column.
        df_price_change (pd.DataFrame): A DataFrame containing monthly price change data with a 'price_change' column.
        path (str): The path to the directory where the CSV file will be saved.
        file_name (str): The name of the CSV file to be saved (excluding the '.csv' extension).

    Returns:
        pd.DataFrame: A summary DataFrame with counts and percentages of 0s and 1s in the 'price_change' column
                      grouped by the 'total' column.

    Example Usage:
        # Load depression values and monthly price changes DataFrames
        depression_df = pd.read_csv('depression_values.csv')
        price_change_df = pd.read_csv('price_changes.csv')

        # Compare depression values with price changes and save the result
        result_summary = compare_depression_with_price_change(depression_df, price_change_df, 'results', 'comparison_result')

    """    
    df = pd.concat([df_depression_value, df_price_change], axis=1)    
    # group the dataframe by the values in column2, and count the occurrences of 0 and 1 in column1
    result_df = df.groupby('total')['price_change'].value_counts().unstack(fill_value=0)    
    # rename the columns to indicate whether the count is for 0 or 1
    result_df.columns = ['count_0', 'count_1']
    result_df['percent_0'] = result_df.apply(lambda x: x['count_0'] / (x['count_0'] + x['count_1']) * 100, axis=1)
    result_df['percent_1'] = result_df.apply(lambda x: x['count_1'] / (x['count_0'] + x['count_1']) * 100, axis=1)

    result_df = result_df.reset_index()
    result_df.to_csv(path+'/'+file_name+'.csv')
    return result_df


def compare_depression_of_2countries(df1:pd.DataFrame, df2:pd.DataFrame, df_price_change:pd.DataFrame, path:str, file_name:str):
    '''
    for creating df1 and df2 dataframes, first we have to use "monthly_features" function and then use "count_depression_value" function.
    for df_price_change dataframe we should use "combine_investing_data", "convert_str_to_float", and "count_monthly_price_change" in order. 
    This function compares two countries' depression rates. The more the depression is, the more powerful that currency is.
    for example, df1 is counted depression rate for EUR and df2 is counted depression rate for USD. If the df1 value is larger than
    the df2 value, the EUR currency should get stronger, so the EUR/USD pair should be bought. Also, if the df2 value is greater than 
    EUR value, the EUR should get weaker, so the EUR/USD pair shoud be sold.
   

    Parameters:
        df1 (pd.DataFrame): The depression rate DataFrame for the first country.
        df2 (pd.DataFrame): The depression rate DataFrame for the second country.
        df_price_change (pd.DataFrame): The DataFrame containing currency price change data.
        path (str): The path to the directory where the CSV file will be saved.
        file_name (str): The name of the CSV file to be saved (excluding the '.csv' extension).

    Returns:
        pd.DataFrame: A summary DataFrame with counts and percentages of price changes based on the comparison of depression rates.

    Example Usage:
        # Load depression rate DataFrames for EUR and USD
        eur_depression_df = monthly_features(files=['EUR_depression'], path='depression_data/')
        usd_depression_df = monthly_features(files=['USD_depression'], path='depression_data/')
        
        # Load currency price change data
        price_change_df = combine_investing_data(path='price_data/', files_name=['EUR_USD'])
        price_change_df = convert_str_to_float(price_change_df)
        price_change_df = count_monthly_price_change(price_change_df)
        
        # Compare depression rates of EUR and USD and save the result
        result_summary = compare_depression_of_2countries(eur_depression_df, usd_depression_df, price_change_df, 'results/', 'depression_comparison')    
    '''
    df = pd.concat([df1, df2, df_price_change], axis=1)
    df['compare_depression'] = np.nan
    for i in range(len(df[df.columns[0]])):
        if df[df.columns[0]][i] >= df[df.columns[1]][i]:
            df['compare_depression'][i] = 1
        else:
            df['compare_depression'][i] = 0
            
    result_df = df.groupby('compare_depression')['price_change'].value_counts().unstack(fill_value=0)    
    result_df.columns = ['count_0', 'count_1']
    result_df['percent_0'] = result_df.apply(lambda x: x['count_0'] / (x['count_0'] + x['count_1']) * 100, axis=1)
    result_df['percent_1'] = result_df.apply(lambda x: x['count_1'] / (x['count_0'] + x['count_1']) * 100, axis=1)   
    result_df.to_csv(path+'/'+file_name+'.csv')
    return result_df       
        

def merge_csv(files_list, saved_file_name='output.xls'):
    """
    Merge multiple CSV files and save the result as an Excel (XLS) file.

    This function takes a list of directories, each containing two CSV files, and merges them into a single Excel file.
    Each directory should contain two CSV files that represent different data sets.

    Parameters:
        files_list (list): A list of directory names containing CSV files to merge.
        saved_file_name (str, optional): The name of the output Excel file. Default is 'output.xls'.

    Returns:
        None

    Example Usage:
        # List of directories containing CSV files
        directories = ['data_set_1', 'data_set_2']

        # Merge CSV files and save the result as 'merged_output.xls'
        merge_csv(directories, 'merged_output.xls')
    """    
    create_folder(path=os.getcwd()+'/feature_analysis/', folder_name='merged_files')
    for file in files_list:
        path = os.getcwd()+'/feature_analysis/'+file+'/'
        csv_files = glob.glob(os.path.join(path, "*.csv"))
        df1 = pd.read_csv(csv_files[0])
        df2 = pd.read_csv(csv_files[1])
        result = pd.concat([df1, df2], axis=1, keys=['actual_target', 'labeled_target'], names=['DataFrame', ''])
        result.to_csv(os.getcwd()+'/feature_analysis/merged_files/'+str(file)+'_merged.csv')  
    
    path = os.getcwd()+'/feature_analysis/merged_files'
    wb = xlwt.Workbook()
    for csvfile in glob.glob(os.path.join(path, '*.csv')):
        fpath = csvfile.split("/", 1)
        fname = fpath[1].split(".", 1)
        fname = fname[0].split("/merged_files/", 1)
        fname = fname[1].split("Historical", 1)
        ws = wb.add_sheet(fname[0][0:20])
        with open(csvfile, 'r') as f:
            reader = csv.reader(f)
            for r, row in enumerate(reader):
                for c, col in enumerate(row):
                    ws.write(r, c, col)
    wb.save(saved_file_name)   
    
    
def news_effect_with_periods(affected_feature_path='../../data/fund_model/energy', affected_feature_file_name='XAU_USD', monthly_news_path='../../data/fund_model/monthly/', monthly_news_file_name= 'Trade Balance', periods=5):
    """
    Analyze the effect of monthly news on a specific feature over multiple periods.

    This function analyzes the effect of monthly news on a selected financial feature over a specified number of periods.
    It calculates the change in the feature's value over each period after the news release and whether it is positive (1) or not (0).

    Parameters:
        affected_feature_path (str, optional): The path to the directory containing the affected financial feature data.
                                              Default is '../../data/fund_model/energy'.
        affected_feature_file_name (str, optional): The name of the affected financial feature file (without extension).
                                                     Default is 'XAU_USD'.
        monthly_news_path (str, optional): The path to the directory containing the monthly news data.
                                           Default is '../../data/fund_model/monthly/'.
        monthly_news_file_name (str, optional): The name of the monthly news file (without extension).
                                                Default is 'Trade Balance'.
        periods (int, optional): The number of periods to analyze after each news release. Default is 5.

    Returns:
        pd.DataFrame: A DataFrame containing the effect of monthly news on the specified feature over multiple periods.

    Example Usage:
        # Analyze the effect of 'Trade Balance' news on 'XAU_USD' over 3 periods
        result_df = news_effect_with_periods(affected_feature_file_name='XAU_USD',
                                             monthly_news_file_name='Trade Balance',
                                             periods=3)
    """
    affected_feature_actual = combine_investing_data(path=affected_feature_path, files_name=[affected_feature_file_name])
    affected_feature_df = convert_str_to_float(affected_feature_actual)
    affected_feature_df = convert_monthly_to_daily(affected_feature_df)
    
    #monthly_news_df = read_forexfactory_data(path=monthly_news_path, files_name=monthly_news_file_name)
    monthly_news_df = monthly_features(files=[monthly_news_file_name], path=monthly_news_path)
    
    column_name_list = ['News_'+monthly_news_file_name]
    for i in range(0, periods+1):
        column_name_list.extend([affected_feature_file_name+'_period_'+str(i)])
    
    new_df = pd.DataFrame(columns=column_name_list)
    for i in range(len(monthly_news_df.index)):
        new_df = new_df.append({'News_'+monthly_news_file_name: monthly_news_df[monthly_news_df.columns[0]].loc[monthly_news_df.index[i]]}, ignore_index=True)
        for n in range(2, periods+3):
            indexes = pd.date_range(start = monthly_news_df.index[i], periods=n) + timedelta(days=-1)
            #print(indexes)
            try:    
                selected_rows = affected_feature_df.loc[indexes[0:n]]
                result = selected_rows[affected_feature_df.columns[0]].iloc[-1] - selected_rows[affected_feature_df.columns[0]].iloc[0]
                if result > 0:        
                    new_df.at[i, affected_feature_file_name+'_period_'+str(n-2)] = 1
                else:
                    new_df.at[i, affected_feature_file_name+'_period_'+str(n-2)] = 0
            except:
                pass
    new_df.set_index(monthly_news_df.index, inplace=True)                
    new_df = new_df.astype(float)
    return new_df


def feature_analysis(path_features:str, files_name:list, power_number:int, path_target:str, target_file_name:str, lags_number:int, path_make_folder=os.getcwd()+'/feature_analysis') -> pd.DataFrame:
    '''
    Inputs:
    path_features: str, path to the directory containing feature files.
    files_name: list of str, names of feature files to be processed.
    power_number: int, highest power to which non-linear features are to be created.
    path_target: str, path to the directory containing target file.
    target_file_name: str, name of target file to be used.
    lags_number: int, number of lags to be created for each feature.
    path_make_folder: str, optional, default value is the current working directory appended with '/feature_analysis', path to the directory where output files will be saved.
    
    Outputs:
    result_df: pandas DataFrame, contains the correlation coefficients of each feature with the target, sorted in descending order of correlation coefficients.
    
    
    The function takes in the paths and names of feature and target files, and creates non-linear features of the given power for the features.
    It then combines the features with the target data, and creates lagged versions of each feature.
    Finally, it calculates the correlation coefficients of each feature with the target for each lag and saves the results in csv files in the specified directory.
    The output DataFrame contains the correlation coefficients of each feature with the target, sorted in descending order of correlation coefficients.
    '''
    for feature_file_name in files_name:
        investing_df = read_investing_daily_data(path=path_features, file_name=feature_file_name)
        investing_bonds_df = convert_str_to_float(investing_df)
        investing_bonds_df_return = return_price(investing_bonds_df)
        investing_bonds_power_df = create_nonlinear_features(df=investing_bonds_df, power_upto=power_number)
        investing_bonds_return_power_df = create_nonlinear_features(df=investing_bonds_df_return, power_upto=power_number)
        investing_bonds_df_exp = exp_function(df=investing_bonds_df)
        investing_bonds_return_df_exp = exp_function(df=investing_bonds_df_return)
        my_df = [investing_bonds_df, investing_bonds_df_return, investing_bonds_power_df,
                 investing_bonds_return_power_df, investing_bonds_df_exp, investing_bonds_return_df_exp]
            
        target_actual = combine_investing_data(path=path_target, files_name=[target_file_name])
        target_labeled = labeling_target(path=path_target, files_name=[target_file_name])  
        my_target = [target_actual, target_labeled]
        target_name = ['target_actual', 'target_labeled']
        create_folder(path=path_make_folder, folder_name=feature_file_name)
    
        for n in range(len(my_target)):
            target_df = convert_str_to_float(my_target[n])
            result_df = pd.DataFrame()
            for i in range(len(my_df)):
                df = my_df[i].join(target_df)
                result_df = get_top_corr_with_gold(df=df, target_file_name=target_file_name).append(result_df)
                for num in range(0, lags_number):
                    try:
                        df.rename(columns = {target_file_name:'target'}, inplace = True)
                        df['target'] = df.target.shift(-num)
                        df.rename(columns = {'target':target_file_name}, inplace = True)
                    except:
                        df.rename(columns = {target_file_name+'_labeled':'target'}, inplace = True)
                        df['target'] = df.target.shift(-num)
                        df.rename(columns = {'target':target_file_name+'_labeled'}, inplace = True)
                    corr_df = get_top_corr_with_gold(df)
                    pre = 'lag'+str(num)+'_'
                    corr_df['feature'] = pre + corr_df['feature'].astype(str)
                    result_df =corr_df.append(result_df)
                    try:
                        result_df = result_df.sort_values(by=target_file_name, ascending=False)
                    except:
                        result_df = result_df.sort_values(by=target_file_name+"_labeled", ascending=False)
                        
            result_df.to_csv('feature_analysis/'+feature_file_name+'/'+str(feature_file_name)+str(target_name[n])+'.csv', index = False)
    return result_df


def comparing(merged_df, state, gold):
    '''
    Compare a binary state feature with gold prices and analyze the results by weekday.

    Parameters:
        merged_df (pd.DataFrame): A DataFrame containing the merged data with features, gold prices, and weekdays.
        state (int): The binary state to be compared (0 or 1).
        gold (int): The binary gold state to be compared (0 or 1).

    Returns:
        pd.DataFrame: A DataFrame containing the comparison results, including percentage mismatches and weekday-specific mismatches.

    The `comparing` function takes a merged DataFrame `merged_df`, a binary `state`, and a binary `gold` as input. It compares the binary `state` feature with gold prices (binary) and analyzes the results by weekday.

    The function calculates the following metrics:
    - `result`: The overall percentage of mismatches between the `state` feature and `gold` prices.
    - `monday`, `tuesday`, `wednesday`, `thursday`, `friday`, `saturday`, `sunday`: The percentage of mismatches for each weekday.

    The results are saved in separate CSV files for each feature in the 'compare' directory.

    Example Usage:
    # Import the necessary libraries
    import pandas as pd

    # Load your merged DataFrame containing features, gold prices, and weekdays
    merged_df = pd.read_csv('merged_data.csv')

    # Define the binary state and gold values to compare
    state = 1
    gold = 0

    # Call the comparing function
    results_df = comparing(merged_df, state, gold)

    # The results_df DataFrame contains the comparison results and can be further analyzed or saved as needed.
    '''    
    feature_list = list(merged_df.columns)
    final_df = pd.DataFrame(columns=['feature', 'state', 'gold', 'result', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday'])
    for feature in feature_list:
        data = {'feature': merged_df[feature], 'gold': merged_df['Price Change XAU_USD'], 'weekday': merged_df['weekday']}
        df = pd.DataFrame(data)
        df.index = merged_df.index
        df = df.dropna()
        final_df['feature'] = [feature]
        final_df['state'] = [state]
        final_df['gold'] = [gold]
        mismatch_rows = ((df['feature'] == state) & (df['gold'] == gold)).sum()
        col1_0_rows = (df['feature'] == 0).sum()
        final_df['result'] = (mismatch_rows / col1_0_rows) * 100
        for i in range(0, 7):
            mismatch_rows = ((df['feature'] == state) & (df['gold'] == gold) & (df['weekday'] == i)).sum()
            col1_0_rows = (df['feature'] == 0).sum()
            final_df[final_df.columns[4+i]] = (mismatch_rows / col1_0_rows) * 100
        final_df.to_csv('compare/compare_'+feature+'.csv') 

def count_ones_zeros(df:pd.DataFrame, path='../../../feature_analysis/news_feature_percent'):
    '''
    Count occurrences and percentages of combinations of binary states in a DataFrame.

    Parameters:
        df (pd.DataFrame): A pandas DataFrame containing binary state columns.
        path (str, optional): Path to the directory where the result CSV file will be saved. Default is '../../../feature_analysis/news_feature_percent'.

    Returns:
        pd.DataFrame: A DataFrame containing counts and percentages of combinations of binary states.

    The `count_ones_zeros` function takes a pandas DataFrame `df` as input. The DataFrame is expected to contain binary state columns (0 or 1).

    The function counts occurrences and calculates percentages for the following combinations:
    - 0_0: Occurrences where both the first column (df.columns[0]) and the column being analyzed have the value 0.
    - 1_1: Occurrences where both the first column (df.columns[0]) and the column being analyzed have the value 1.
    - 0_1: Occurrences where the first column (df.columns[0]) has the value 0 and the column being analyzed has the value 1.
    - 1_0: Occurrences where the first column (df.columns[0]) has the value 1 and the column being analyzed has the value 0.

    The results are stored in a DataFrame and saved as a CSV file in the specified directory. The resulting DataFrame is also returned.

    Example Usage:
    # Import the necessary libraries
    import pandas as pd

    # Create a sample DataFrame with binary state columns
    data = {'Column1': [1, 0, 1, 0, 0, 1],
            'Column2': [0, 1, 1, 0, 0, 1],
            'Column3': [1, 1, 0, 0, 1, 1]}
    df = pd.DataFrame(data)

    # Call the count_ones_zeros function
    result_df = count_ones_zeros(df)
    '''
    
    data = []
    for i in range(len(df.columns)):
        count_0_0 = len(df[(df[df.columns[0]] == 0) & (df[df.columns[i]] == 0)])
        count_1_1 = len(df[(df[df.columns[0]] == 1) & (df[df.columns[i]] == 1)])
        count_0_1 = len(df[(df[df.columns[0]] == 0) & (df[df.columns[i]] == 1)])
        count_1_0 = len(df[(df[df.columns[0]] == 1) & (df[df.columns[i]] == 0)])
        
        percent_0_0 = count_0_0 / len(df) * 100
        percent_1_1 = count_1_1 / len(df) * 100
        percent_0_1 = count_0_1 / len(df) * 100
        percent_1_0 = count_1_0 / len(df) * 100
        
        data.append({'column': df.columns[i], 'count_0_0': count_0_0, 'count_1_1': count_1_1, 'count_0_1': count_0_1, 'count_1_0': count_1_0, 
                     'percent_0_0': percent_0_0, 'percent_1_1': percent_1_1, 'percent_0_1': percent_0_1, 'percent_1_0': percent_1_0})
        
    percent_df = pd.DataFrame(data)
    percent_df.to_csv(path+'/'+df.columns[0]+'.csv')
    return percent_df

def find_relation(file_name: list, path_files: str, target_path: str, target_file_name: str):
    '''
    Count occurrences and percentages of combinations of binary states in a DataFrame.

    Parameters:
        df (pd.DataFrame): A pandas DataFrame containing binary state columns.
        path (str, optional): Path to the directory where the result CSV file will be saved.

    Returns:
        pd.DataFrame: A DataFrame containing counts and percentages of combinations of binary states.

    The `count_ones_zeros` function takes a pandas DataFrame `df` as input. The DataFrame is expected to contain binary state columns (0 or 1).

    The function counts occurrences and calculates percentages for the following combinations:
    - 0_0: Occurrences where both the first column (df.columns[0]) and the column being analyzed have the value 0.
    - 1_1: Occurrences where both the first column (df.columns[0]) and the column being analyzed have the value 1.
    - 0_1: Occurrences where the first column (df.columns[0]) has the value 0 and the column being analyzed has the value 1.
    - 1_0: Occurrences where the first column (df.columns[0]) has the value 1 and the column being analyzed has the value 0.

    The results are stored in a DataFrame and saved as a CSV file in the specified directory. The resulting DataFrame is also returned.

    Example Usage:
    # Import the necessary libraries
    import pandas as pd

    # Create a sample DataFrame with binary state columns
    data = {'Column1': [1, 0, 1, 0, 0, 1],
            'Column2': [0, 1, 1, 0, 0, 1],
            'Column3': [1, 1, 0, 0, 1, 1]}
    df = pd.DataFrame(data)

    # Call the count_ones_zeros function
    result_df = count_ones_zeros(df)
    '''
    data=[]
    df_target = read_investing_data(path=target_path, file_name=target_file_name)
    for file in file_name:
        df_feature = read_investing_data(path=path_files, file_name=file)
        merged_df = pd.concat([df_feature, df_target], axis=1)
        df = merged_df.dropna()
        #for lag uncomment next line
        df[df.columns[0]] = df[df.columns[0]].shift(-1)
        
        count_0_0 = len(df[(df[df.columns[0]] == 0) & (df[df.columns[1]] == 0)])
        count_1_1 = len(df[(df[df.columns[0]] == 1) & (df[df.columns[1]] == 1)])
        count_0_1 = len(df[(df[df.columns[0]] == 0) & (df[df.columns[1]] == 1)])
        count_1_0 = len(df[(df[df.columns[0]] == 1) & (df[df.columns[1]] == 0)])

        percent_0_0 = count_0_0 / (df[df.columns[0]]==0).sum() * 100
        percent_1_1 = count_1_1 / (df[df.columns[0]]==1).sum() * 100
        percent_0_1 = count_0_1 / (df[df.columns[0]]==0).sum() * 100
        percent_1_0 = count_1_0 / (df[df.columns[0]]==1).sum() * 100
        
        data.append({'column': df.columns[0], 'count_0_gold-': count_0_0, 'count_1_gold+': count_1_1, 'count_0_gold+': count_0_1, 'count_1_gold-': count_1_0, 
             'percent_0_gold-': percent_0_0, 'percent_1_gold+': percent_1_1, 'percent_0_gold+': percent_0_1, 'percent_1_gold-': percent_1_0})

    percent_df = pd.DataFrame(data) 
    percent_df.to_csv(path_files+'/'+'laged_result'+'.csv')
    return percent_df    
